{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto final do curso Spark - Big Data Processing Semantix Academy\n",
    "\n",
    "### Campannha Nacional de Vacinação contra Convid-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação das bibliotecas\n",
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F\n",
    "from  pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Enviar os dados para o hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando os dados no hdfs\n",
    "#!hdfs dfs -put /input/covid_br/ /user/allan/projeto_covid_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rw-r--r--   3 root supergroup   62492959 2022-04-21 20:00 /user/allan/projeto_covid_br/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   3 root supergroup   76520681 2022-04-21 20:00 /user/allan/projeto_covid_br/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rw-r--r--   3 root supergroup   91120916 2022-04-21 20:00 /user/allan/projeto_covid_br/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rw-r--r--   3 root supergroup    3046774 2022-04-21 20:00 /user/allan/projeto_covid_br/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    }
   ],
   "source": [
    "# Consultando os dados no hdfs\n",
    "!hdfs dfs -ls /user/allan/projeto_covid_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regiao;estado;municipio;coduf;codmun;codRegiaoSaude;nomeRegiaoSaude;data;semanaEpi;populacaoTCU2019;casosAcumulado;casosNovos;obitosAcumulado;obitosNovos;Recuperadosnovos;emAcompanhamentoNovos;interior/metropolitana\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-25;9;210147125;0;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-26;9;210147125;1;1;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-27;9;210147125;1;0;0;0;;;\r",
      "\r\n",
      "Brasil;;;76;;;;2020-02-28;9;210147125;1;0;0;0;;;\r",
      "\r\n",
      "cat: Unable to write to output stream.\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando o formato dos dados\n",
    "!hdfs dfs -cat /user/allan/projeto_covid_br/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv | head -n 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30010;53001;DISTRITO FEDERAL;2020-07-22;30;3015268;87801;1725;1176;18;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-23;30;3015268;90023;2222;1218;42;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-24;30;3015268;92414;2391;1244;26;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-25;30;3015268;94187;1773;1275;31;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-26;31;3015268;96332;2145;1308;33;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-27;31;3015268;98480;2148;1339;31;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-28;31;3015268;100726;2246;1391;52;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-29;31;3015268;102342;1616;1419;28;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-30;31;3015268;104442;2100;1444;25;;;1\r",
      "\r\n",
      "Centro-Oeste;DF;Brasília;53;530010;53001;DISTRITO FEDERAL;2020-07-31;31;3015268;106292;1850;1469;25;;;1\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -tail /user/allan/projeto_covid_br/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Otimizar todos os dados do hdfs para uma tabela Hive particionada por município"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conectando com o hive-server\n",
    "- docker exec -it hive-server bash\n",
    "\n",
    "Conectando com o beeline\n",
    "- beeline -u jdbc:hive2://localhost:10000\n",
    "\n",
    "Criando o banco\n",
    "- create database allan\n",
    "- use allan\n",
    "\n",
    "Criando uma tabela\n",
    "- create table covid_br(regiao String, estado String, municipio String, coduf int, codmun int, codRegiaoSaude int, \n",
    "nomeRegiaoSaude String, data date, semanaEpi int, populacaoTCU2019 int, casosAcumulado int, casosNovos int,\n",
    "obitosAcumulado int, obitosNovos int, Recuperadosnovos int, emAcompanhamentoNovos int, interiorMetropolitana int)\n",
    "row format delimited\n",
    "fields terminated by ';'\n",
    "lines terminated by '\\n'\n",
    "stored as textfile\n",
    "tblproperties(\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "Descrição da tabela\n",
    "- desc formatted covid_br;\n",
    "\n",
    "Carregando os dados do hdfs para a tabela criada\n",
    "- load data inpath '/user/allan/projeto_covid_br' overwrite into table covid_br;\n",
    "\n",
    "Visualizando os dados \n",
    "- select * from covid_br limit 10;\n",
    "\n",
    "Contar o numero de registros\n",
    "- select count(*) from covid_br;\n",
    "- 2624943\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Devido a grande quantidade de municipios e por limitação da maquina utilizada, o particionamento por municipios nao foi possivel. Dessa forma iremos utilizar a partição por estado\n",
    "\n",
    "particionamento dinamico\n",
    "\n",
    "create table covid_br_particao_UF(regiao String,municipio String,coduf int,codmun int,codRegiaoSaude int,nomeRegiaoSaude String,data date,semanaEpi int,populacaoTCU2019 int,casosAcumulado int,casosNovos int,obitosAcumulado int,obitosNovos int,\n",
    "Recuperadosnovos int,emAcompanhamentoNovos int,interiorMetropolitana int)\n",
    "\n",
    "partitioned by (estado String)\n",
    "\n",
    "row format delimited\n",
    "\n",
    "fields terminated by ';'\n",
    "\n",
    "lines terminated by '\\n'\n",
    "\n",
    "stored as textfile\n",
    "\n",
    "tblproperties(\"skip.header.line.count\"=\"1\");\n",
    "\n",
    "Inserindo os dados da tabela particionada\n",
    "\n",
    "INSERT OVERWRITE TABLE covid_br_particao_UF PARTITION(estado)\n",
    "\n",
    "SELECT regiao,municipio,coduf,codmun,codRegiaoSaude,nomeRegiaoSaude,data,semanaEpi,populacaoTCU2019,casosAcumulado,\n",
    "casosNovos,obitosAcumulado,obitosNovos,Recuperadosnovos,emAcompanhamentoNovos,interiorMetropolitana,estado\n",
    "FROM covid_br;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28 items\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=AC\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=AL\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=AM\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=AP\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=BA\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=CE\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=DF\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=ES\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=GO\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=MA\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=MG\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=MS\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=MT\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=PA\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=PB\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=PE\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=PI\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=PR\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=RJ\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=RN\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=RO\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=RR\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=RS\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=SC\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=SE\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=SP\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=TO\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:23 /user/hive/warehouse/allan.db/covid_br_particao_uf/estado=__HIVE_DEFAULT_PARTITION__\n"
     ]
    }
   ],
   "source": [
    "# Acessando os dados salvos no hdfs\n",
    "\n",
    "!hdfs dfs -ls /user/hive/warehouse/allan.db/covid_br_particao_uf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "-rwxrwxr-x   3 root supergroup   62492959 2022-04-21 11:57 /user/hive/warehouse/allan.db/covid_br/HIST_PAINEL_COVIDBR_2020_Parte1_06jul2021.csv\r\n",
      "-rwxrwxr-x   3 root supergroup   76520681 2022-04-21 11:57 /user/hive/warehouse/allan.db/covid_br/HIST_PAINEL_COVIDBR_2020_Parte2_06jul2021.csv\r\n",
      "-rwxrwxr-x   3 root supergroup   91120916 2022-04-21 11:57 /user/hive/warehouse/allan.db/covid_br/HIST_PAINEL_COVIDBR_2021_Parte1_06jul2021.csv\r\n",
      "-rwxrwxr-x   3 root supergroup    3046774 2022-04-21 11:57 /user/hive/warehouse/allan.db/covid_br/HIST_PAINEL_COVIDBR_2021_Parte2_06jul2021.csv\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/allan.db/covid_br"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = spark.read.csv(\"/user/hive/warehouse/allan.db/covid_br\", sep=\";\", header=\"True\", inferSchema=\"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(regiao='Brasil', estado=None, municipio=None, coduf=76, codmun=None, codRegiaoSaude=None, nomeRegiaoSaude=None, data=datetime.datetime(2020, 2, 25, 0, 0), semanaEpi=9, populacaoTCU2019=210147125, casosAcumulado=Decimal('0'), casosNovos=0, obitosAcumulado=0, obitosNovos=0, Recuperadosnovos=None, emAcompanhamentoNovos=None, interior/metropolitana=None),\n",
       " Row(regiao='Brasil', estado=None, municipio=None, coduf=76, codmun=None, codRegiaoSaude=None, nomeRegiaoSaude=None, data=datetime.datetime(2020, 2, 26, 0, 0), semanaEpi=9, populacaoTCU2019=210147125, casosAcumulado=Decimal('1'), casosNovos=1, obitosAcumulado=0, obitosNovos=0, Recuperadosnovos=None, emAcompanhamentoNovos=None, interior/metropolitana=None)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|regiao|estado|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|               data|semanaEpi|populacaoTCU2019|casosAcumulado|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interior/metropolitana|\n",
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-25 00:00:00|        9|       210147125|             0|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-26 00:00:00|        9|       210147125|             1|         1|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-27 00:00:00|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-28 00:00:00|        9|       210147125|             1|         0|              0|          0|            null|                 null|                  null|\n",
      "|Brasil|  null|     null|   76|  null|          null|           null|2020-02-29 00:00:00|        9|       210147125|             2|         1|              0|          0|            null|                 null|                  null|\n",
      "+------+------+---------+-----+------+--------------+---------------+-------------------+---------+----------------+--------------+----------+---------------+-----------+----------------+---------------------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: timestamp (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: decimal(10,0) (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(covid.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo oarquivo inferindo schema\n",
    "estrutura_lista = [\n",
    "    StructField(\"regiao\", StringType()),\n",
    "    StructField(\"estado\", StringType()),\n",
    "    StructField(\"municipio\", StringType()),\n",
    "    StructField(\"coduf\", IntegerType()),\n",
    "    StructField(\"codmun\", StringType()),\n",
    "    StructField(\"codRegiaoSaude\", StringType()),\n",
    "    StructField(\"nomeRegiaoSaude\", StringType()),\n",
    "    StructField(\"data\", DateType()),\n",
    "    StructField(\"semanaEpi\", IntegerType()),\n",
    "    StructField(\"populacaoTCU2019\", IntegerType()),\n",
    "    StructField(\"casosNovos\", IntegerType()),\n",
    "    StructField(\"obitosAcumulado\", IntegerType()),\n",
    "    StructField(\"obitosNovos\", IntegerType()),\n",
    "    StructField(\"Recuperadosnovos\", IntegerType()),\n",
    "    StructField(\"emAcompanhamentoNovos\", IntegerType()),\n",
    "    StructField(\"interiorMetropolitana\", IntegerType())\n",
    "]\n",
    "\n",
    "schema_names = StructType(estrutura_lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid = spark.read.csv(\"/user/hive/warehouse/allan.db/covid_br_particao_uf\", sep=\";\", schema=schema_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+-----+------+--------------+---------------+----+---------+----------------+----------+---------------+-----------+----------------+---------------------+---------------------+------+\n",
      "|regiao|municipio|coduf|codmun|codRegiaoSaude|nomeRegiaoSaude|data|semanaEpi|populacaoTCU2019|casosNovos|obitosAcumulado|obitosNovos|Recuperadosnovos|emAcompanhamentoNovos|interiorMetropolitana|estado|\n",
      "+------+---------+-----+------+--------------+---------------+----+---------+----------------+----------+---------------+-----------+----------------+---------------------+---------------------+------+\n",
      "|  null|     null| null|  null|          null|           null|null|     null|            null|      null|           null|       null|            null|                 null|                 null|    MG|\n",
      "|  null|     null| null|  null|          null|           null|null|     null|            null|      null|           null|       null|            null|                 null|                 null|    MG|\n",
      "|  null|     null| null|  null|          null|           null|null|     null|            null|      null|           null|       null|            null|                 null|                 null|    MG|\n",
      "|  null|     null| null|  null|          null|           null|null|     null|            null|      null|           null|       null|            null|                 null|                 null|    MG|\n",
      "|  null|     null| null|  null|          null|           null|null|     null|            null|      null|           null|       null|            null|                 null|                 null|    MG|\n",
      "+------+---------+-----+------+--------------+---------------+----+---------+----------------+----------+---------------+-----------+----------------+---------------------+---------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "covid.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: string (nullable = true)\n",
      " |-- codRegiaoSaude: string (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: date (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interiorMetropolitana: integer (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(covid.printSchema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['regiao',\n",
       " 'municipio',\n",
       " 'coduf',\n",
       " 'codmun',\n",
       " 'codRegiaoSaude',\n",
       " 'nomeRegiaoSaude',\n",
       " 'data',\n",
       " 'semanaEpi',\n",
       " 'populacaoTCU2019',\n",
       " 'casosNovos',\n",
       " 'obitosAcumulado',\n",
       " 'obitosNovos',\n",
       " 'Recuperadosnovos',\n",
       " 'emAcompanhamentoNovos',\n",
       " 'interiorMetropolitana',\n",
       " 'estado']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covid.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Criar 3 visualizações pelo Spark com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+----------------+---------------+\n",
      "|sum(Recuperadosnovos)|sum(obitosNovos)|sum(casosNovos)|\n",
      "+---------------------+----------------+---------------+\n",
      "|              1580655|       274777592|     9998172092|\n",
      "+---------------------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual3 = covid.agg({'Recuperadosnovos': 'sum', 'casosNovos': 'sum','obitosNovos': 'sum'})\n",
    "visual3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sum(Recuperadosnovos)', 'sum(obitosNovos)', 'sum(casosNovos)']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Recuperados', 'Obitos', 'Novos']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visual3_ = visual3.withColumnRenamed('sum(Recuperadosnovos)', 'Recuperados')\\\n",
    "    .withColumnRenamed('sum(obitosNovos)', \"Obitos\")\\\n",
    "    .withColumnRenamed('sum(casosNovos)', \"Novos\")\n",
    "\n",
    "visual3_.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Salvar a primeira visualização como tabela hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+--------------------------+\n",
      "|sum(Recuperadosnovos)|sum(emAcompanhamentoNovos)|\n",
      "+---------------------+--------------------------+\n",
      "|               523208|                2920055795|\n",
      "+---------------------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual4 = covid.agg({'Recuperadosnovos': 'sum', 'emAcompanhamentoNovos': 'sum'})\n",
    "\n",
    "visual4.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Recuperados|Acompanhamento|\n",
      "+-----------+--------------+\n",
      "|     523208|    2920055795|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual4_ = visual4.withColumnRenamed('sum(Recuperadosnovos)', 'Recuperados')\\\n",
    "    .withColumnRenamed('sum(emAcompanhamentoNovos)', \"Acompanhamento\")\n",
    "\n",
    "visual4_.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='allan', description='', locationUri='hdfs://namenode:8020/user/hive/warehouse/allan.db'),\n",
       " Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listando os bancos no hive\n",
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 11:57 /user/hive/warehouse/allan.db/covid_br\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:11 /user/hive/warehouse/allan.db/covid_br_particao\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:28 /user/hive/warehouse/allan.db/covid_br_particao_municipio\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:24 /user/hive/warehouse/allan.db/covid_br_particao_uf\r\n"
     ]
    }
   ],
   "source": [
    "# Listando as tabelas existentes\n",
    "!hdfs dfs -ls /user/hive/warehouse/allan.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Can not create the managed table('`exe4`'). The associated location('hdfs://namenode:8020/user/hive/warehouse/exe4') already exists.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o88.saveAsTable.\n: org.apache.spark.sql.AnalysisException: Can not create the managed table('`exe4`'). The associated location('hdfs://namenode:8020/user/hive/warehouse/exe4') already exists.;\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:170)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:465)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:444)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:400)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-3398d7788f0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisual4_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exe4\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Can not create the managed table('`exe4`'). The associated location('hdfs://namenode:8020/user/hive/warehouse/exe4') already exists.;\""
     ]
    }
   ],
   "source": [
    "visual4_.write.saveAsTable(\"exe4\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxrwxr-x   - root supergroup          0 2022-04-21 19:27 /user/hive/warehouse/allan.db\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-24 21:02 /user/hive/warehouse/exe4\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-24 22:50 /user/hive/warehouse/exe5\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-10 13:00 /user/hive/warehouse/juros2\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando se os dados foram salvos\n",
    "!hdfs dfs -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-24 21:02 /user/hive/warehouse/exe4/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        724 2022-04-24 21:02 /user/hive/warehouse/exe4/part-00000-7b3228e7-f9c0-4841-bd09-cf8177656fad-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse/exe4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------+\n",
      "|Recuperados|Acompanhamento|\n",
      "+-----------+--------------+\n",
      "|     523208|    2920055795|\n",
      "+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando os dados salvos no hive\n",
    "exe4 = spark.read.parquet(\"/user/hive/warehouse/exe4/part-00000-7b3228e7-f9c0-4841-bd09-cf8177656fad-c000.snappy.parquet\")\n",
    "exe4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Salvar a segunda visualização com formato parquet e compressão snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+\n",
      "|   populacao|casosNovos|\n",
      "+------------+----------+\n",
      "|307861595007|9987481628|\n",
      "+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual5 = covid.agg({'casosNovos': 'sum','populacaoTCU2019': 'sum'})\n",
    "visual5 = visual5.withColumnRenamed('sum(casosNovos)', 'casosNovos').withColumnRenamed('sum(populacaoTCU2019)', 'populacao')\n",
    "visual5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculando a letalidade: (número de óbitos x 100) / número de casos confirmados.\n",
    "\n",
    "Calculando a mortalidade: (óbitos * 1.000.000) / população.\n",
    "\n",
    "Calculando a incidencia: (casos confirmados * 1.000.000) / população"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a incidencia - (casos confirmados * 1.000.000) / população\n",
    "def incidencia(populacao, casosNovos):\n",
    "    resp = (casosNovos*1000000) / populacao\n",
    "    return resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float datatype is defined\n",
    "new_f = F.udf(incidencia, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual5 = visual5.withColumn(\"incidencia\",\n",
    "                          new_f(\"populacao\", \"casosNovos\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- populacao: long (nullable = true)\n",
      " |-- casosNovos: long (nullable = true)\n",
      " |-- incidencia: float (nullable = true)\n",
      "\n",
      "+------------+----------+----------+\n",
      "|   populacao|casosNovos|incidencia|\n",
      "+------------+----------+----------+\n",
      "|307861595007|9987481628| 32441.467|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing and printing the schema of the Dataframe\n",
    "visual5.printSchema()\n",
    "visual5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "\"Can not create the managed table('`exe5`'). The associated location('hdfs://namenode:8020/user/hive/warehouse/exe5') already exists.;\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o152.saveAsTable.\n: org.apache.spark.sql.AnalysisException: Can not create the managed table('`exe5`'). The associated location('hdfs://namenode:8020/user/hive/warehouse/exe5') already exists.;\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)\n\tat org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:170)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:668)\n\tat org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:465)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:444)\n\tat org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:400)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1832bf079050>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvisual5\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"exe5\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"overwrite\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msaveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0msince\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: \"Can not create the managed table('`exe5`'). The associated location('hdfs://namenode:8020/user/hive/warehouse/exe5') already exists.;\""
     ]
    }
   ],
   "source": [
    "visual5.write.saveAsTable(\"exe5\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-24 22:50 /user/hive/warehouse/exe5/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        924 2022-04-24 22:50 /user/hive/warehouse/exe5/part-00000-06fc8048-588d-4c6e-a7c0-6efa8cef95b2-c000.snappy.parquet\r\n"
     ]
    }
   ],
   "source": [
    "# verificando se foi salvo\n",
    "!hdfs dfs -ls /user/hive/warehouse/exe5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+----------+\n",
      "|   populacao|casosNovos|incidencia|\n",
      "+------------+----------+----------+\n",
      "|307861595007|9987481628| 3244.1467|\n",
      "+------------+----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando os dados salvos no hive\n",
    "exe5 = spark.read.parquet(\"/user/hive/warehouse/exe5/part-00000-06fc8048-588d-4c6e-a7c0-6efa8cef95b2-c000.snappy.parquet\")\n",
    "exe5.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Salvar a terceira visualização em um topico no Kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+\n",
      "|   populacao|  obitos|casosNovos|\n",
      "+------------+--------+----------+\n",
      "|307861595007|56523691|9987481628|\n",
      "+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual6 = covid.agg({'casosNovos': 'sum', 'obitosAcumulado': 'sum', 'populacaoTCU2019': 'sum'})\n",
    "visual6 = visual6.withColumnRenamed('sum(casosNovos)', 'casosNovos')\\\n",
    "            .withColumnRenamed('sum(obitosAcumulado)', 'obitos')\\\n",
    "            .withColumnRenamed('sum(populacaoTCU2019)', 'populacao')\n",
    "            \n",
    "visual6.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a letalidade - (número de óbitos x 100) / número de casos confirmados.\n",
    "def letalidade(obitos, casosNovos):\n",
    "    letal = (obitos * 100) / casosNovos\n",
    "    return letal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define mortalidade - (óbitos * 1.000.000) / população\n",
    "def mortalidade(obitos, populacao):\n",
    "    mortal = (obitos * 100000) / populacao\n",
    "    return mortal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# float datatype is defined\n",
    "new_f6 = F.udf(letalidade, FloatType())\n",
    "new_f7 = F.udf(mortalidade, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "visual7 = visual6.withColumn(\"Letalidade\",\n",
    "                             new_f6(\"obitos\", \"casosNovos\"))\n",
    "visual8 = visual7.withColumn(\"Mortalidade\",\n",
    "                            new_f7(\"obitos\", \"populacao\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- populacao: long (nullable = true)\n",
      " |-- obitos: long (nullable = true)\n",
      " |-- casosNovos: long (nullable = true)\n",
      " |-- Letalidade: float (nullable = true)\n",
      " |-- Mortalidade: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Showing and printing the schema of the Dataframe\n",
    "visual8.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+----------+-----------+\n",
      "|   populacao|  obitos|casosNovos|Letalidade|Mortalidade|\n",
      "+------------+--------+----------+----------+-----------+\n",
      "|307861595007|56523691|9987481628| 0.5659454|  18.360098|\n",
      "+------------+--------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salvando a visualização no hdfs\n",
    "visual8.write.json(\"/user/allan/visual-spark-kafka\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-09 18:00 /user/allan/data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-16 11:51 /user/allan/juros_selic\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-15 13:47 /user/allan/logs_count_word\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-15 13:46 /user/allan/logs_count_word3\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-15 14:03 /user/allan/logs_count_word_5\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-15 14:36 /user/allan/names_us_orc\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-21 11:13 /user/allan/projeto\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-21 20:00 /user/allan/projeto_covid_br\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-17 14:51 /user/allan/projeto_python\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-16 12:25 /user/allan/relatorio_anual\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-17 16:21 /user/allan/stream\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-16 11:01 /user/allan/teste.csv\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-25 19:06 /user/allan/visual-spark-kafka\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-04-25 16:32 /user/allan/visual-spark-kafka.json\r\n"
     ]
    }
   ],
   "source": [
    "# Verificando se o arquivo foi salvo\n",
    "!hdfs dfs -ls /user/allan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "-rw-r--r--   2 root supergroup          0 2022-04-25 19:06 /user/allan/visual-spark-kafka/_SUCCESS\r\n",
      "-rw-r--r--   2 root supergroup        116 2022-04-25 19:06 /user/allan/visual-spark-kafka/part-00000-47c99506-8b5f-43fa-98c2-4c274443479c-c000.json\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/allan/visual-spark-kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enviando o arquivo para o HDFS\n",
    "# hdfs dfs -get /user/allan/visual-spark-kafka /input\n",
    "\n",
    "# enviando o arquivo do HDFS para o container do kafka\n",
    "# docker cp /home/allan/docker/spark/docker-bigdata/input/visual-spark-kafka kafka:/home\n",
    "\n",
    "# Criar um topico\n",
    "# kafka-topics.sh --bootstrap-server kafka:9092 --topic KafkaSpark --create --partitions 1 --replication-factor 1\n",
    "\n",
    "# Listar os topicos\n",
    "# kafka-topics.sh --bootstrap-server kafka:9092 --list\n",
    "\n",
    "# criar o consumidor\n",
    "# kafka-console-consumer.sh --bootstrap-server kafka:9092 --topic KafkaSpark -- group topicArquivo\n",
    "\n",
    "# criar um produtor com o arquivo\n",
    "# kafka-console-producer.sh --broker-list kafka:9092 --topic KafkaSpark < /home/visual-spark-kafka/part-00000-bcea425b-bfc5-4f2a-9cde-0ed48ffa900b-c000.json\n",
    "\n",
    "# Resultado exibido na tela do consumidor\n",
    "# {\"populacao\":307861595007,\"obitos\":56523691,\"casosNovos\":9987481628,\"Letalidade\":565.9454,\"Mortalidade\":18.360098}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Criar a visualização pelo Spark com os dados enviados para o HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+--------+----------+\n",
      "|      regiao|   populacao|  obitos|casosNovos|\n",
      "+------------+------------+--------+----------+\n",
      "|    Nordeste| 55070808753| 8889245|1630021092|\n",
      "|        null|        null|    null|      null|\n",
      "|         Sul| 28926824560| 7215160|1148363624|\n",
      "|     Sudeste| 85278432845|14268253|2414880019|\n",
      "|Centro-Oeste| 15726676410| 3833238| 715156998|\n",
      "|      Brasil|105073562500|18855015|3343282900|\n",
      "|       Norte| 17785289939| 3462780| 735776995|\n",
      "+------------+------------+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sintese de casos, obitos, incidenciam e mortalidade por região\n",
    "visual9 = covid.groupBy(\"regiao\").agg({'casosNovos': 'sum', 'obitosAcumulado': 'sum', 'populacaoTCU2019': 'sum'})\n",
    "\n",
    "visual9 = visual9.withColumnRenamed('sum(casosNovos)', 'casosNovos')\\\n",
    "            .withColumnRenamed('sum(obitosAcumulado)', 'obitos')\\\n",
    "            .withColumnRenamed('sum(populacaoTCU2019)', 'populacao')\n",
    "            \n",
    "visual9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Salvar a visualização do exercicio 6 em um topico no Elastic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+--------+----------+----------+-----------+\n",
      "|   populacao|  obitos|casosNovos|Letalidade|Mortalidade|\n",
      "+------------+--------+----------+----------+-----------+\n",
      "|307861595007|56523691|9987481628| 0.5659454|  18.360098|\n",
      "+------------+--------+----------+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "visual8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
